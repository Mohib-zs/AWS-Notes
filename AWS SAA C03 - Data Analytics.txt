Amazon Athena is a serverless query service to analyze data stored in Amazon S3. It uses standard SQL language to query the files (built on Presto), supports CSV, JSON, ORC, Avro and Parquet. Pricing is $5.00 per TB of data scanned and is commonly used with Amazon Quicksight for reporting/dashboards. Use cases are Business Intelligence/Analytics/reporting and for analyzing and querying any kind of logs that originate from your AWS services. For EXAM: if you're asked to analyze data in S3 using serverless SQL engine, use Athena.

To improve Athena performance, use columnar data for cost saving, using Parquet or ORC format is recommended for less scan time. use Glue to convert data to Parquet or ORC via ETL for data analytics. Compress data for smaller retrievals, Partition datasets in S3 for easy querying on virtual columns using attributes like year/month/day to retrieve a subset of data that falls in that classification. Use large files (> 128MB) to minimize overhead as many small files can bottleneck performance.

Athena Federated query allows you to run SQL queries across data stored in relational, non-relational object and custom data sources (AWS/On-prem). Uses Data source connectors that run on Lambda to run federated queries (e.g CloudWatch Logs, DynamoDB, RDS..) and store the results back in S3.

Redshift is a PostgreSQL based database, but its not used for OLTP. Instead its OLAP - Online Analytical Processing (analytics and data warehousing) 10x better performance than other data warehouses, scale to PBs of data. Columnar storage of data (data stored in columns not rows) & paralled query engine. Two modes, provisioned cluster or serverless cluster, Has SQL interface for performing queries. Used with BI tools such as Quicksight or Tableau.

Redshift has faster queries/joins/aggregations as compared to Athena thanks to indexes but requires you to provision a whole cluster while Athena is serverless and all your data lives in S3.

Redshift has a leader node for aggregation and compute node for the actual querying. It has multi-az mode for some clusters, snapshots are PITR backups of cluster stored in S3, snapshots are incremental (only what has changed is saved). Automated or manual snapshots, and can configure Redshift to auto copy snapshots of a cluster to another AWS Region for DR.

Use Large ingests to load data to Redshift, an example is Firehose storing data in S3 and then data from S3 is loaded to Redshift cluster via S3 copy (via Internet of through VPC endpoints). You can also use a JDBC driver to send application data running in EC2 to redshit and its much better to write that data in batches rather than in rows one-by-one.

To query data that is already in S3 without loading it, one must have a RedShift cluster available to start the query. It is then submitted to thousands of Redshift spectrum nodes which perform aggregation on data in S3 and then send result back to RedShift compute nodes. In FROM attribute, query starts with S3.xyz if querying directly from S3.

Amazon OpenSearch is successor to Amazon Elasticsearch. In DynamoDB, queries only exist by primary key or indexes. With OpenSearch, you can search any field, even partial matches. Its common to use OpenSearch as a complement to another Database. Two modes, cluster and serverless. Does not natively support SQL (can be enabled via a plugin). Ingestion from Kinesis Data Firehose, AWS IoT, and CloudWatch Logs. Security via Cognito & IAM, KMS encryption, TLS. Comes with OpenSearch Dashboards (visualization). OpenSearch can do near-real-time pattern searching with K.Data Streams throwing data to firehose (with Lambda performing transformations optionally) and then sends data to OpenSearch. It can also do real-time data pattern search with Lambda function processing and sending data to OpenSearch from Kinesis Data streams or DynamoDB streams.

EMR stands for Elastic MapReduce. EMR helps in creating Hadoop clusters (Big Data) to analyze and process vast amount of data (or Big Data). The cluster can be made of hundreds of EC2 instances. EMR comes bundled with Apache Spark, HBase, Presto, Flink and EMR takes care of all the provisioning of the infra and the configuration of these difficult-to-setup services. Auto scaling integrated with sopt instances for cost reduction. Used in data processing, ML, web indexing but mainly with technologies related to Big Data (if exam says big data, think EMR). EMR consists of the master node (to manage the cluster, coordinate, managae health and its long running), Core node (to run tasks and store data - long running) and Task node (optional, just to run tasks and is usually Spot). For purchasing you can go On-demand, Reserved or Spot Instances. You can have a long running cluster or temporary (transient) cluster. 

Amazon QuickSight is a serverless ML-powered BI service to create interactive dashboards. Its fast, auto-scalable, embeddable, with per-session pricing. Use cases are Business analytics, Building visualizations, Perform ad-hoc analysis or getting business insights using data. Its integrated with RDS, Aurora, Athena, Redshift, S3, Opensearch and Timestream ; Also integrates with 3rd party SaaS Data sources like Jira/Salesforce ; also with on-prem Databases using JDBC protocol and can also import data in form of xlsx, csv, json, .tsv or ELF/CLF (log format). You can use SPICE engine for in-memory computation if you directly import data to Quicksight rather than connecting it with a database. You can also have the possibility to setup Column Level Security (CLS) with quicksight enterprise edition to prevent some columns from being displayed to certain users. In Quicksight you define users (standard) or groups (enterprise) in quicksight, not IAM!! A dashboard is a read-only snapshot of your analysis that you can share with users or groups which also preserves config like parameters/arguments but you must first publish it, and he who can see dashboard can also see underlying data.

AWS Glue is a data processing ETL tool which extracts data from datasource (S3/RDS) and, performs transformation (if needed, like add some columns or convert data format from .csv to parquet) and then load the data to a data warehouse like RedShift, or if in parquet, load it to a S3 output bucket where Athena can do analysis on it (easier for athena to do analysis on parquet since its in columnar format). This can also be automated by using Lambda or eventbridge to trigger glue ETL job whenever a S3 PUT operations occurs on the input S3 bucket. You can also use AWS Glue Data Crawler to crawl in your databases like S3, RDS, DynamoDB, on-prem JDBC db etc. This crawler collects the metadata of the databases content like datatypes and send it to AWS Glue Data Catalog which also has databases with tables filled with those metadata. The catalog can then perform Glue Jobs on that metadata. Additionally, this metadata can also be used by Athena, Redshift or EMR for their operations on that data as well. Glue Job Bookmarks prevent re-processing of old data, Glue Elastic Views combine and replicate data across multiple data stores using SQL with no custom code. Glue DataBrew provides clean and normalize data using pre-built transformation, Glue Studio provides new GUI to create, run and monitor ETL jobs in Glue. Glue Streaming ETL (built on Apache Spark structed streaming) is compatible with Kinesis data streams, Kafka, MSK (managed kafka)

AWS Lake Formation is a data lake (a central place to have all your data stored for analytics purposes). It is a layer built on top of glue and is a Fully managed service that makes it easy to setup a data lake (stored in S3) in days, Discover, cleanse, transform and ingest data into your data lake. It automates many complex manual steps (collecting, cleansing, moving, cataloging data) and de-duplicate (using ML transforms). Combine structured and unstructured data in the data lake, has out-of-the-box source blueprints for datasources such as S3, RDS, NoSQL DBs etc. Fine-grained access control for apps at row and column level. It ingests data from datasources and the datalake has source crawlers, ETL and data prep, data catalog, security and access control. Services like Athena, RedShift, EMR connect to data lake to perform analytics and users connect to these services. Main benefit of AWS Lake Formation is "Centralized Permissions or Security" which is also a keyword. Rather than setting up access policies at data source level or at athena or quicksight which can become messy, with Lake Formation you only setup access control (column level) at Lake Formation and all connected to Lake Formation (RDS, NoSQL, S3, Athena with Quicksight, RedShift or EMR) will have their access control managed accordingly.

Amazon Managed Service for Apache Flink is  a managed framework (Java, Scala or SQL) for processing data streams. It ingests data from Data Streams such as Kinesis Data Streams or Amazon MSK (Managed Apache Kafka). This service allows you to run any Apache Flink application on a managed cluster on AWS, it provisions compute resources, parallel computation, auto scaling with application backups as checkpoints/snapshots. Use any Apache Flink programming features to transform data. IMP: FLINK DOES NOT READ FROM DATA FIREHOSE, ONLY STREAMS.

Amazon Managed Streaming for Kafka is an alternative to Kinesis. Fully managed Kafka cluster on AWS which allows you to create, update and delete clusters. MSK creates and manages Kafka broker nodes & Zookeeper nodes for you. Deploy the MSK cluster in your VPC, multi-AZ (upto 3 for HA). Auto recovery from common Kafka failures and Data is stored in EBS vols for as long as you want. Makes managing Kafka much easier. You can also go serverless on MSK to run Kafka without managing capacity, MSK auto provisions resources and scales compute & Storage. IMP: Main Difference is message size limit, KINESIS IS 1MB MAX WHILE MSK IS 1MB DEFAULT AND 10MB MAX. Streams uses shards to auto-scale while MSK uses Kafka topics for partitioning and scaling. Both have TLS in-flight and KMS at rest encryption. MSK also has PLAINTEXT encryption in-flight. Amazon MSK can have Kinesis Data Analytics for Flink, AWS Glue (Streaming ETL jobs powered by Apache Spark streaming), Lambda and apps running on EC2, ECS, EKS.

IoT core is a service in AWS which allows us to manage IoT devices. These devices send data to IoT core in real-time. Its later sent to Kinesis Data streams for further processing.

The ideal Big Data Ingestion pipeline should be serverless, collect data in real-time, transform the data, allows us to query the transformed data in SQL, store the reports created by the queries in S3 and then load that data into a warehouse and create dashboards.

A solution is to use AWS IoT core which allows us to harvest data from IoT devices in real-time, then use Kinesis Data Streams which is great for real-time data collection and piping, then use Firehose  to ingest that data and deliver it to S3 (input) in near real-time (1 min is minimum frequency). Lambda can help Firehose to perform data transformations before it is delivered to S3 (input). (e.g convert .csv to parquet or remove missing columns). S3 can trigger notifications to SQS and Lambda can subscribe to SQS (or just connect Lambda to S3 as well). Use Athena as a serverless SQL service to query the stored data in S3 (input) and store the reports in S3 (output). The S3 output bucket containing the reports of analyzed data can be used by reporting tools such as AWS QuickSight, EMR or Redshift to perform further analytics on it.

MCQS Points

If you have been instructed you to prepare a disaster recovery plan for a Redshift cluster, enable Automated Snapshots, then configure your Redshift cluster to automatically copy snapshots to another AWS region

Enhanced VPC routing in Redshift forces all COPY and UNLOAD traffic moving between your cluster and data repositories through your VPCs

OpenSearch can centrally store those logs and efficiently search and analyze those logs in real-time for detection of any errors and if there is a threat