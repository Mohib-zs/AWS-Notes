 Amazon S3 is a core part of Amazon, It is used for hosting of websites as well as an integration to other AWS services and is commercialized as infinitely scaling storage. Amazon S3 Usecases are Backup and storage, DR, Archive, Hybrid Cloud storage, Application hosting, Media hosting, Data lakes & big data analytics, software delivery and static website. S3 allows people to store objects (files) in buckets (directories). Buckets must have a globally unique name (across all regions all accounts), they are defined at the region level. S3 looks like a global service but buckets are created in a region. Naming convention is No uppercase, No underscore, 3-63 characters long, Not an IP, Must start with lowercase letter or number, Must no start with prefix xn-- and must not end with the suffix -s3alias.

Objects (files) have a Key which is the FULL path: "s3://my-bucket/my_file.txt" or if the object is nested in folders then "s3://my-bucket/my_folder1/another_folder/my_file.txt". There's no concept of "directories" within buckets (although the UI will trick you otherwise) they're just keys with very long names and contain slashes (/). Object values are content of the body, max object size is 5TB and if uploading a file > 5GB, then use "multi-part upload", like for 5TB, you upload 1000 separate parts of 5GB of the file. Metadata (list of text key/value pairs - system or user metadata) Tags (Unicode key/value pair - up to 10) - useful for security / lifecycle, version ID (if versioning is enabled).

You can choose to enable or disable ACLs for security as well as block public access or not, for encryption you use server-side encryption with amazon s3 managed keys (SSE-S3) or with AWS KMS keys (SSE-KMS) or Dual-layer server-side encryption with AWS Key Management Service keys (DSSE-KMS). Pre-signed URL is a public URL with the owner's or admin users credentials, token attached with the public URL for access to a object which has public access blocked.

Amazon S3 security is either user-based with IAM Policies - which API calls should be allowed for a specific user from IAM or resource based using bucket policies (most common) which provide bucket wide rules from the s3 console for cross account access. Object ACL provide finer grain security along with Bucket ACL (less common), both can be disabled. IAM principal can access an S3 object if the user IAM permissions ALLOW it OR the resource policy ALLOWS it AND theres no explicit DENY. Encrypt objects in Amazon S3 using encryption keys. Bucket policy is a JSON based policy with buckets and objects as resources, effect allow/deny, actions to set API to allow or deny, Principal is the accounts or users to apply policy to (* means all), Use S3 bucket for policy to grant public access to bucket, Force objects to be encrypted at upload or grant access to another account (cross account).

In summary, use bucket policies (assigned to bucket) if you want anonymous internet users to access your bucket , Use IAM policies (assigned to users) when you want users in your AWS account to access your bucket and use IAM roles (assigned to resources) if you want an AWS resource (like EC2) to access your bucket. For cross account access, use bucket policy which allows cross account access for that specific IAM user and the user will be able to make API calls to the bucket. Use policy generator if you have difficulty writing the JSON policy yourself.

For static hosting on S3, enable it from the S3 bucket settings, you would have to provide a index.html file to represent the home page. URL of the website looks like <<bucket-name.s3-website-region.amazonaws.com>> or <<bucket-name.s3-website.region.amazonaws.com>>

Versioning your files on S3 is done at the bucket level, same key overwrite will change the version 1,2,3 and its best practice to version your buckets to protect against unintended deletes (ability to restore a version) and easy rollback to previous version. Also any file that is not versioned prior to version enablement will have version null and suspending versioning will not delete the previous versions.

Deleting an object with versioning enabled/disabled will have two scenarios:

If versioning is disabled and you delete an object, the object is permanently deleted.

If versioning is enabled and you delete an object, a delete marker is added. The original object (including the null version, if it exists) is not deleted — it’s just hidden which can be restored if you delete the permanent marker. For permanent delete, you must specify the version id as well of the object.

Replication in S3 can be CRR (Cross Region Replication) and SRR (Same Region Replication). You must enable versioning in source and destination buckets, Buckets can be in different AWS accounts and the copying is asynchronous though you must give proper IAM permissions to S3. CRR is used when requirements are compliance, lower latency, and replication across accounts. SRR is for log aggregation, live replication between production and test accounts. Only new objects are replicated but optionally, you can replicate existing objects or objects that failed replication using S3 Batch Replication. You can replicate delete markers from source to destination (optional setting) but not deletions with a version ID. No chaining of replication meaning if bucket 1 replicates to bucket 2 and 2 has to 3, contents of 1 wont get replicated to 3.

S3 promises 99.99999999999% durability and 99.5 -  99.99% availabilty and has many storage classes, they are defined below:

S3 Standard - General Purpose:
99.99 % Availability, Used for frequently accessed data, Low latency and high throughput, Sustain 2 concurrent facility failures. Usecases are Big data analytics, mobile & gaming apps, content distribution..

S3 - Infrequent Access
For data that is less frequently accessed, but requires rapid access when needed, has lower cost than S3 standard, two tiers of S3 - Infrequent access
 
S3 Standard - Infrequent Access (S3 Standard-IA)
99.9% Availability
Use cases: Disaster Recovery, backups

S3 One Zone-Infrequent Access (S3 One Zone-IA)
High Durability in a single az; data lost when AZ is destroyed, 99.5% Availability, Usecases are storing secondary backup copies of on-premise data, or data you can recreate.

S3 Glacier Storage Classes
Low-cost object storage meant for archiving/backup, pricing is storage+object retrieval cost, three tiers of it:

S3 Glacier Instant Retrieval:
Millisecond retrieval, great for data accessed once a quarter and minimum storage duration is 90 days

S3 Glacier Flexible Retrieval
Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours - free) and minimum storage duration is 90 days.

S3 Glacier Deep Archive - for long term storage:
Standard (12 hours), Bulk (48 hours) and minimum storage duration of 180 days. 

S3 Intelligent-Tiering
Small monthly monitoring and auto-tiering fee, Move objects automatically between access tiers based on usage, there are no retrieval charges in S3 Intelligent-Tiering

Frequent Access tier (automatic): default tier
Infrequent Access tier (automatic): objects not accessed for 30 days
Archive Instant Access tier (automatic): objects not accessed for 90 days
Archive Access tier (optional): configurable from 90 days to 700+ days
Deep Archive Access tier (optional): configurable from 180 days to 700+ days

You can make custom storage tier transition rules for objects by creating lifecycle rules. Like you can move an object (current/noncurrent version) to Standard IA 60 days after creation or move to glacier after 6 months. Configure objects (current/noncurrent version) to expire (delete) after some time, access log files to be deleted after 365 days, or deletion of old version of files and their delete markers as well as incomplete multi part uploads. Rules can be applied to specific path of s3 bucket url (e.g s3://mybucket/mp3/*) or specific objects tags (e.g env: dev)

One way to analyze if transistioning is required is through Amazon S3 Analytics - storage class analysis which helps you decide when to transition objects to the right storage class. Recommendations for Standard and Standard IA, doesnt work for One-Zone IA or Glacier. S3 Bucket data goes through S3 Analytics that creates a .csv report which is updated daily but you must wait 24 to 48 hours to start seeing data analysis. So a .csv report is a good first step to put together life cycle rules and improve them.

S3 also has a requester pays feature. Usually the bucket owner is the one who pay for all Amazon S3 storage and data transfer costs associated with their bucket, But with requester pays buckets, the one who requests for the contents of the bucket will pay the cost of the request and data download from the bucket. Its helpful when sharing large datasets with other accounts. The requester must be authenticated in AWS (can't be anonymous).

S3 has event notifications feature, events can be S3:ObjectCreated, S3:ObjectRemoved etc. Object name filtering is possible like filter objects with .jpg. Usecase is to react to an event like create thumbnails of all the .jpg images uploaded to S3 and send them to other target services, supported ones are SNS, SQS, Lambda and Amazon EventBridge (uses rules to send notifications to over 18 AWS services as targets). Create as many S3 events and send them to targets of your choice. Event notifications are typically delieverd in seconds but can sometimes take a minute. The S3 bucket can only access the target services (SNS,SQS,Lambda & Eventbridge) if a Resource (Access) Policy is attached to the service which allows the bucket to send data to it. IAM Roles are'nt used in this case, resource policy is best option.

S3 automatically scales to high request rates and has 100-200 ms lag when pushing out the first byte when request made. App can acheive atleast 3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket, (object path = prefix): e.g bucket/folder1/sub1/file (prefix = /folder1/sub1/) or bucket/1/file (/1/ prefix), spread across the two prefixes, you get 11000 GET/HEAD requests. Optimize S3 performance using multi-part upload (recommened for files > 100MB and must for files >5GB). It helps parallelize uploads (speed up transfers). Another way  to increase transfer speed is by transfering to an AWS edge location which will forward the data to the S3 bucket in the target region (compaitaible with multi-part upload) allows you minimize public data usage and maximize private AWS data usage (super-fast).

For fast downloads you can parallelize GETs by requesiting specific byte ranges, provides better resilience in case of failures. Like 100 MB file, you make 200 byte specific range and all the requests will be made in parallel. Can also be used to retrieve only partial data like first 50 bytes (which maybe the HEAD of the file)

S3 Batch operations is a feature which allows to perform bulk operations on existing S3 objects with a single request, example:
Modify object metadata & properties,
Copy objects between S3 buckets,
Encrypt un-encrypted objects
Modify ACLs, tags,
Restore objects from S3 Glacier, 
Invoke Lambda function to perform custom action on each object.

A job consists of a list of objects, the action to perform, and optional parameters. S3 batch operations manages retires, tracks progress, send completion notifications and generate reports. You can use S3 Inventory to get object list and use Athena to query the list and filter your objects, which could then be sent to S3 batch operations with the operation to be performed + parameters and get processed objects as result.


S3 storage lens allows you to understand, analyze and optimize storage across entire AWS Organization, discover anomalies, identify costs efficiencies and apply data protection best practices across entire AWS Organization (30 days usage & activity metrics). Aggregate data for Organization, specific accounts, regions, buckets or prefixes. Default/custom dashboards and can be configured to export metrics daily to an S3 bucket (CSV, Parquet). Default dashboard visualizes summarized insights and trends for both free and advanced metrics. Default dashboard shows multi-region and multi-account data and is preconfigured by Amazon S3, can't be detected but can be disabled.

Metrics are of two types, one is summary metrics which is general insights about your S3 storage like StorageBytes and ObjectCount, use cases are to identify the fastest-growing (or not used) buckets and prefixes. Second is cost-optimization metrics which provide insights to manage and optimize your storage costs like NonCurrentVersionStorageBytes, IncompleteMultipartUploadStorageBytes etc. Usecases are identify buckets with incomplete multipart uploaded older than 7 days, identify which objects could be transitioned to lower-cost storage class. You have data-protection merics, access management metrics, event metrics for S3 notification insights, performance metrics for s3 transfer acceleration, activity metrics abour how your storage is requesed and detailed status code metrics to provide insights for HTTP status codes.

Free metrics are auto available for all customers, contain around 28 usage metrics and data is available for queries for 14 days. Advanced metrics and recommendations hold additional paid metrics and features, activity, advanced cost optimization, advanced data protection and status code. cloudwatch publishing and prefix aggregation (gather metrics at prefix level) and data is available for query for 15 months. 