Serverless is a paradigm where developers dont manage servers anymore, they just deploy code or functions. Intially Serverless was aka FaaS (Function as a Service) It wa pioneered by AWS Lambda but now also includes anything that's remotely managed like databases, messaging, storage etc. Serverless means you dont manage, provision and see servers. A usecases of Lambda is Users calling Rest API via AWS API Gateway to invoke Lambda to pull data from DB and present to users. Some example serverless services are Lambda, DynamoDB, Cognito, API Gateway, S3, SNS & SQS, Kinesis Data Firehose, Aurora Serverles, Step Functions, Fargate.

Lambda are virtual functions not servers, they are limited by time (used for short event based executions, min 3 sec and max is 15 min before timeout), they run on-demand, scaling is automated. Benefits of Lambda is easy pricing (you pay per request and compute time), free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time. Integrated with the whole AWS suite of services, Integraed with many programming languages, Easy monitoring through AWS CloudWatch, Easy to get more resources per functions (min size of function is 128 MB and goes upto 10GB of RAM per function) and increasing the RAM will also improve CPU and network. Environment variables can be stored of upto 4KB, but you may use Disk capacity in the function container (in /tmp):512 MB to 10GB. 1000 concurrent executions provided and can be increased. Lambda function for deployment size (compressed .zip) is 50MB while uncompressed deployment (code + dependencies) is 250MB, use /tmp directory to load other files at startup with more disk space and size of environment variables is 4KB.

AWS Lambda supports langs like Node.js (JavaScript), Python, Java, C# (.NET Core) / Powershell, Ruby, Custom Runtime API (community supported, example Rust or Golang). Also supports docker images but they must implement the Lambda Runtime API. ECS/Fargate is preferred for running arbitary Docker images.

Main integrations of Lambda with AWS services are API Gateway to create a REST API to invoke Lambda functions, Kinesis to perform transformations on the fly, DynamoDB to perform a function on each DB event, S3 to execute function on any event like file change, CloudFront for Lambda at edge usage, CloudWatch Events/ Eventbridge to execute some automated step whenever something occurs in our Infra like state change in code pipeline, CloudWatch Logs to stream logs wherever we want, SNS to react to notifications in your SNS topics, SQS to process messages from your SQS queues, Cognito to react whenever a user logs into our DB.

e.g is to create a serverless cron job by attaching cloudwatch eventbridge that will be triggered every one hour and every one hour it will be integrated with the Lambda function to perform your tasks.

If the exam questions you on things like, 30 min of execution is required or 30GB RAM is required or a 3GB big file is needed, know that Lambda is not the right way.

Lambda provides 1000 concurrent executions but you can set reserved concurrency at the function level (=limit). Each invocation over the concurrency limit will trigger a Throttle. Throttle behaviour for synchronous invocation = return ThrottleError - 429 while Asynchronous behaviour will retry and then go to DLQ. If you need a limit higher than 1000, raise a support ticket to AWS. The 1000 Lambda function limit is attached at the account level. e.g A usecase to understand concurrency issue is that, suppose we have a ALB with Lambda for app 01, API Gateway with Lambda for App 02 and SDK/CLI for lambda for app 03, if ALB faces huge surge and Lambda executions goes upto 1000, that is good for APP 01, but APP 02 and APP03 using Lambda will get throttled as they would not be able to perform executions.

 Concurrency with async invocations run that if the function doesnt have enough concurrency available to process all events, additional requests are throttled. For throttling errors (429) and sytem errors (500-series), Lambda returns the event to the queue and attempts to run the function again for upto 6 hours. The retry interval increases after the first attempt to a maximum of 5 minutes.

Cold Start is when you run the first instance of Lambda on your code and it takes some time, thats because code is first loaded outside the handler run (init) If the init is large (code, dependencies,SDK) this process can take some time. First request is served by new instances has higher latency than the rest. Solution for this is provisioned concurrency where concurrency is allocated befre the function is invoked, so cold start never happens. Application Auto Scaling manages concurrency (schedule or target utilization). Cold Starts have dramitaclly reduced since Oct/Nov 2019.



