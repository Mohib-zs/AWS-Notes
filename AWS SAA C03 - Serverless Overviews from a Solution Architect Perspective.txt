Serverless is a paradigm where developers dont manage servers anymore, they just deploy code or functions. Intially Serverless was aka FaaS (Function as a Service) It wa pioneered by AWS Lambda but now also includes anything that's remotely managed like databases, messaging, storage etc. Serverless means you dont manage, provision and see servers. A usecases of Lambda is Users calling Rest API via AWS API Gateway to invoke Lambda to pull data from DB and present to users. Some example serverless services are Lambda, DynamoDB, Cognito, API Gateway, S3, SNS & SQS, Kinesis Data Firehose, Aurora Serverles, Step Functions, Fargate.

Lambda are virtual functions not servers, they are limited by time (used for short event based executions, min 3 sec and max is 15 min before timeout), they run on-demand, scaling is automated. Benefits of Lambda is easy pricing (you pay per request and compute time), free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time. Integrated with the whole AWS suite of services, Integraed with many programming languages, Easy monitoring through AWS CloudWatch, Easy to get more resources per functions (min size of function is 128 MB and goes upto 10GB of RAM per function) and increasing the RAM will also improve CPU and network. Environment variables can be stored of upto 4KB, but you may use Disk capacity in the function container (in /tmp):512 MB to 10GB. 1000 concurrent executions provided and can be increased. Lambda function for deployment size (compressed .zip) is 50MB while uncompressed deployment (code + dependencies) is 250MB, use /tmp directory to load other files at startup with more disk space and size of environment variables is 4KB.

AWS Lambda supports langs like Node.js (JavaScript), Python, Java, C# (.NET Core) / Powershell, Ruby, Custom Runtime API (community supported, example Rust or Golang). Also supports docker images but they must implement the Lambda Runtime API. ECS/Fargate is preferred for running arbitary Docker images.

Main integrations of Lambda with AWS services are API Gateway to create a REST API to invoke Lambda functions, Kinesis to perform transformations on the fly, DynamoDB to perform a function on each DB event, S3 to execute function on any event like file change, CloudFront for Lambda at edge usage, CloudWatch Events/ Eventbridge to execute some automated step whenever something occurs in our Infra like state change in code pipeline, CloudWatch Logs to stream logs wherever we want, SNS to react to notifications in your SNS topics, SQS to process messages from your SQS queues, Cognito to react whenever a user logs into our DB.

e.g is to create a serverless cron job by attaching cloudwatch eventbridge that will be triggered every one hour and every one hour it will be integrated with the Lambda function to perform your tasks.

If the exam questions you on things like, 30 min of execution is required or 30GB RAM is required or a 3GB big file is needed, know that Lambda is not the right way.

Lambda provides 1000 concurrent executions but you can set reserved concurrency at the function level (=limit). Each invocation over the concurrency limit will trigger a Throttle. Throttle behaviour for synchronous invocation = return ThrottleError - 429 while Asynchronous behaviour will retry and then go to DLQ. If you need a limit higher than 1000, raise a support ticket to AWS. The 1000 Lambda function limit is attached at the account level. e.g A usecase to understand concurrency issue is that, suppose we have a ALB with Lambda for app 01, API Gateway with Lambda for App 02 and SDK/CLI for lambda for app 03, if ALB faces huge surge and Lambda executions goes upto 1000, that is good for APP 01, but APP 02 and APP03 using Lambda will get throttled as they would not be able to perform executions.

 Concurrency with async invocations run that if the function doesnt have enough concurrency available to process all events, additional requests are throttled. For throttling errors (429) and sytem errors (500-series), Lambda returns the event to the queue and attempts to run the function again for upto 6 hours. The retry interval increases after the first attempt to a maximum of 5 minutes.

Cold Start is when you run the first instance of Lambda on your code and it takes some time, thats because code is first loaded outside the handler run (init) If the init is large (code, dependencies,SDK) this process can take some time. First request is served by new instances has higher latency than the rest. Solution for this is provisioned concurrency where concurrency is allocated befre the function is invoked, so cold start never happens. Application Auto Scaling manages concurrency (schedule or target utilization). Cold Starts have dramitaclly reduced since Oct/Nov 2019.

Unreserved concurrency is now lower than 1000 (e.g 50) though AWS raises quotas based on usage, you may request for quota increase. Reserving concurrency is important for a function, if reserved is 0 then function will be always throttled.

Lambda SnapStart improves your Lambda functions performance upto 10x at no extra cost for Java, Python & .NET. When enabled, function is invoked from a pre-intialized state. (no function intialization from scratch). When you publish a new version, Lambda intializes your function, takes a snapshot of memory and disk state of the intialized function and snapshot is cached for low-latency access.

Customization at the edge is where many modern apps execute some form of logic at the edge. An edge function is a code that you write and attach to CloudFront distributions, runs close to your users to minimize latency. CloudFront provides two types: CloudFront Functions & Lambda@Edge. No need to manage any servers, deployed globally. Use case is to customize the CDN content and pay only for what you use. Usecases are web security/privacy, Dynamic web app at edge, SEO, Intelligent routing across origins/data centers, Real-time image transformation, A/B testingm User Auth, Tracking and analytics.

CloudFront Functions are lightweight functions written in JS for high scale and lag sensitive CDN customizations. Sub-ms startup times, millions of requests/second. Used to change viewer requests and responses: viewer request is after CloudFront receives a request from the viewer which we can modify or the viewer response is to modify before CloudFront forwards the response back to the viewer. Native feature of CloudFront where entire code is managed directly.

Lambda functions written in NodeJS or Python, it scales to 1000s of requests/second. Used to change/modify CloudFront requests and responses: this includes viewer request and response (like CLFront functions) and origin request (before cloudwatch forwards request to origin) and origin response after CloudFront receives the response from the origin. You must author your functions in one AWS Region (us-east-1), then cloudFront replicates to its locations. 

Lambda by default launches in AWS owned VPC not yours, can't access your private resources like RDS, Elasticache or internal DB, only public like DynamoDB. However, to launch Lambda in your VPC, you must define the VPC ID, Subnets, and the Security groups, Lambda will then create an ENI (Elastic Network Interface) in your subnets and connect to the ENI to access your internal/private VPC resources. Issues is here that Lambda functions directly access your database, they may open too many connections under high load slowing down the DB instance. RDS Proxy improves scalability by pooling and sharing DB connections, improves availability by reducing the failover time by 66% and preserving connections, improves security by enforcing IAM auth and storing creds in secrets manager. Lambda function must be deployed in your VPC because RDS proxy is never publicly accessible. 

Invoking Lambda functions from your DB instance is also possible but only for RDS for PostgreSQL and Aurora MySQL. This allows you to process data events from within a database. Though you must allow outbound traffic to your Lambda function from within your DB instance (Public, NAT GW, VPC endpoints). DB instance must have the required permissions to invoke the Lambda function (Lambda resource-based Policy and IAM Policy). e.g is when user inserts account data to DB by creating account and DB invokes Lambda function to send a welcome0 email to user via Amazon SES.

RDS event notifications tells information about the DB instance itself (create, start ,stopped) but you don't have any information about the data itself. Subscribe to the following event categories: DB instance, DB snapshot, DB parameter group, DB security group, RDS Proxy, Custom Engine version. Gives near real-time events (up to 5 minutes). It can send notification to SNS or subscribe to events using eventbridge, both of which can later invoke the Lambda function.


Amazon DynamoDB is a fully managed, highly available DB with replication across multiple AZs. NoSQL database, not a relational database and has transaction support. Millions of requests per seconds, trillions of row, 100s of TB of storage. ITs fast and consistent in performance (single-digit millisecond speed) and its integrated with IAM for security, authorization and administration. Its low cost and auto-scalling capabilities, cloud-native and requires no maintainence or patching, always available. It has standard and Infrequent access (IA) table class.

DynamoDB is made of Tables, each table has a primary key (must be decided at creation time) and each table can have an infinite number of items (=rows). Each item has attributes (can be added over time - can be null) max size of an item is 400KB, data types supported are: Scalar types - string, Number, Binary, Boolean, Null ; and Document types - List, Map ; Set Types - String Set, Number Set, Binary Set. "FOR EXAM": Great choice if your schema (how your data looks, how it's shaped) needs to rapidly evolve. In that case, DynamoDB is a much better choice than Aurora or RDS so look for this at the exam.

In DynamoDB, Primary key consists mainly the Partition Key and optionally the Sort Key (e.g User_ID, Game_ID) and second is the attributes (e.g Score, Result) attributes can be null and you can add atttributes overtime.

DynamoDB allows you to manage your table's capacity (read/write throughput). Provisioned mode (default) allows you to specify the number of reads/writes per second, you need to plan capacity beforehand, pay for provisioned read capacity units (RCU) and write capacity units (WCU). Possibility to add autoscaling mode for RCU & WCU. Great for predictable workloads which evolves smoothly and you wanna do cost-saving.

On-Demand mode allows your reads/writes to auto scale up/down with your workloads. No capacity planning needed, pay for what you use, means more expensive ($$$) great for unpredictable workloads, steep sudden spikes. Used if you have 4-5 transaction a day max because you only pay for what you use.

DynamoDB Accelerator (DAX) is a fully-managed, HA, seamless in-memory cache for DynamoDB. It helps solve read congestion by caching, microseconds latency for cached data and doesn't require application logic modification (compaitible with existing DynamoDB APIs). The cache has a 5 minute TTL. DAX cluster is the middleware b/w application and DynamoDB cluster connected to both. With DynamoDB, DAX cluster is used for individual objects cache and query & scan cache while Amazon Elasticache is used for Storing aggregation result after massive computation (which you may have run on top of DynamoDB). Though DAX Cluster is used with DynamoDB is most usecases.
