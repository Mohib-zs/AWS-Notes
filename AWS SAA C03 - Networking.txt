You can have upto 5 VPC in an AWS region (soft limit) Min Size of VPC is /28 (256 ips) and max is /16(65536 ips). 5 IP address are reserved by AWS in each subnet, first 4 (e.g 10.0.0.0 network address, 10.0.0.1 vpc router, 10.0.0.2 AWS DNS mapping, 10.0.0.3 future use, 10.0.0.255 Network Broadcast).

Exam Tip is if you need 29 IP addresses for EC2 then don't choose /27 that gives 32 IPs since only 27 can be used. Use /26 for 64 ips.

For internet access EC2 connects to route tables, which routes to router, which routes to IGW. Connect public subnets with public route tables and private subnets with private route tables.

NAT instance must be in public subnet with fixed elastic ip to connect with private subnet and must disable source/destination check settings. Route tables of private subnet must be configured as well to allow the traffic flow to NAT instance.

NAT Gateway (better than NAT instance) is AWS Managed, higher bandwidth and HA, no administration. Pay per hour usage and bandwidth based. uses fixed Elastic IP and provisioned in specific az, for HA, provision multiple NAT GW in multi-az. Though there's no need to connect az because if the az A goes down than the instances also go down with NAT GW, but az b is still running. Requires an IGW, (Private subnet => NATGW => IGW) 5 Gbps bandwidth and autoscaling to 100Gbps, No SG to manage

Security group is stateful, meaning whatever is allowed inbound (e.g traffic on port 22) is automatically allowed outbound as well even if its not defined in the SG outbound rules, and vice-versa is for outbound to inbound. Multiple SG can be connected to the EC2 instance's ENI. SG allows all outbound by default, and inbound is at user choosing (mainly 22, 80 and 443 is allowed.)

NACL is stateless at subnet level, meaning both inbound traffic and outbound traffic will be evaluated independently. Only one NACL per subnet. Rule number which is lesser will have higher priority. Default NACL will allow all traffic by default but custom NACL will deny all traffic by default. Each Subnet comes with a default NACL. Create a custom NACL and then add rules, dont change default one.

Ephemeral ports are ports opened by the client for a specific connection session only to receive response from server after making request to it, windows range is 49150 - 65500 while linux is 32700 - 60999. Between to app tier subnets (public web subnet and private db subnet) public subnet will allow outbound to db on 3306, and private will allow inbound on 3306. However for outbound (which would be a ephmenral port) a range of 1024 - 65535 ports must allowed outbound on private and inbound on public.

VPC peering is used to privately connect multiple VPCs using AWS network, make them behave as if they were in the same network basically. They MUST NOT have overlapping CIDRs. VPC Peering connection is NOT transitive meaning if we have VPC A, B, C then connection should be A=>B : B=>C : A=>C. All must be connected to one another. Route tables must also be updated to allow instances in different subnets of different VPCs to communicate with one another. VPC Peering connections can be made cross account/region. You can reference a SG in a peered VPC (great because now source doesn't have to be a CIDR, works for cross-accounts same region)

VPC Endpoints (AWS PrivateLink) allows us to connect to AWS Services (SNS, DynamoDB, S3) using private networks instead of public internet (Because every AWS service is publicly exposed) They're redundant and scale horizontally, they remove the need of IGW, NATGW to access AWS services. In case of issues check DNS resolution settings and route tables in your VPC.

Two types of endpoints, Interface endpoints (by PrivateLink) provisions an ENI with private ipv4 as an entry point, attach a SG to it to allow traffic from ec2 in private subnet, than ec2 goes from the endpoint to the service. Supports most AWS service and is costly. Second is Gateway Endpoint which provisions a NAT gateway and must be used as a target in route table (can't use SG) supports only S3 and DynamoDB and is FREE. Gateway endpoint will be more preferred option at the exam. Interface endpoint will only be applied if we need to connect from on-prem via site-to-site or direct connect, or connect from different VPC/Region.

VPC Flow logs capture info about IP traffic going into your interfaces, this includes VPC flow logs, subnet flow logs, ENI flow logs. Helps to monitor and troubleshoot connectivity issues. Flow logs data can go to S3, CloudWatch Logs and Kinesis Data Firehose. Captures network info from AWS managed interfaces too, e.g ELB, RDS, ElastiCache, RedShift, WorkSpaces, NATGW, Transit Gateway. Used for analytics for usage patterns or malicious behavior. Query VPC flow logs using Athena on S3 or CloudWatch Logs Insights.

To connect an on-prem data center network with your AWS VPC network, configure Customer gateway (CGW)(software or physical device) on on-prem network and VPN Gateway (VGW) on AWS VPC network, then use site-to-site (S2S) VPN connection to connect them together over the internet encrypted. CGW will either use Public IP of the gateway device, if not then it will use the NAT device's public IP (using NAT-T) for the connection. Must enable route propagation for the VGW in the route table of subnets. To ping EC2 from on-prem ensure that ICMP protocol is added on the inbound of your security groups. VPN CloudHub is used to provide secure communication b/w multiple VPN connections (CGWs) to your VGW. Ensure that dynamic routing and route tables are configured.

Direct Connect (DX) provides a dedicated private connection from remote network to your VPC. Dedicated connection must be setup b/w your DC and AWS Direct Connect locations. Setup VGW on your VPC. Allows access to both public/private AWS resources over private network. Increased bandwidth and great for real-time data feeds and has lower cost. Great for hybrid environments and supports IPv4/IPv6. You can also setup Public virtual Interface or VIF with DX to directly connect with AWS e.g S3 without a VGW. To connect to multiple AWS VPCs via DX, the Direct Connect Gateway must have its VIF be configured to connect to both VPC.

Direct Connect dedicated connection ranges from 1Gbps to 100Gbps capacity. Physical ethernet ports are dedicated to customer but request is first made to AWS then AWS DX partners handle things. Second is hosted connections that range from 50Mbps to 10Gbps, request made to AWS DX partners and capacity is added/removed on demand. Lead times to establish a new DX connection takes longer than 1 month, so if exam asks that data connection must be made fast within "1 week", then DX is not the answer. Data in transit in DX not encrypted by default but is private. Use VPN alongside DX that provides IPsec-encryption private network. Good feels for extra security but a bit complex. High resiliency is acheived by having two DX location each with its own VIF to the datacenter. Maximum resiliency is acheived by having two connections in each DX location to the on-prem datacenters (four connection if two DX and on-prem). Incase Direct Connection fails, you can setup backup DX connection (expensive) or setup S2S Backup connection over the internet as backup with primary connection being DX.

AWS Transit Gateway is used for having transitive peering between thousands of VPC and on-prem (as VPC peering can get messy), using the Star topology or hub-and-spoke topology. Regional resource but can work cross-region. Share cross-account using Resource Access Manager and you can peer Transit Gateways across regions. Route Tables are used to limit which VPC can talk with other VPC. Works with Direct Connect Gateway and VPN connections. Supports "IP Multicast" (not supported by other AWS services) so if exam says IP multicast, answer is Transit Gateway. Increase S2S VPN bandwidth using ECMP by connecting the on-prem VPN with Transit Gateway rather than a VPC. Increase the connections number of VPN to increase bandwidth further. Cost will be increased as Transit Gateway charges per gb of throughput. Share Direct Connect between multiple accounts by making the DX connection to Transit Gateway and routing it to two VPC in two accounts.

VPC Traffic mirroring allows you to capture and inspect network traffic in your VPC. Route the traffic to security appliances that you manage, capture the traffic from source (ENI) and targets (ENI or NLB) capture all packets or capture the packets of your interest. Source and Target can be in same VPC or different VPC connected via peering. Use cases are content inspection and threat detecting, troubleshooting.

IPv4 is always enabled in VPC and subnet but IPv6 is not, you can use Ipv6 as public IP for EC2 but not private. IPv4 cannot be disabled so if you cannot launch an EC2 in your subnet, its because there are no available IPv4 subnet space left. Solution is to create a new IPv4 CIDR in your subnet.

Egress-only IGW is like NAT GW but for IPv6. Outbound is allowed but no inbound so instance with IPv6 in private subnet can do egress but no ingress. Route tables must be updated. 

AWS Interface endpoint (GW endpoint) > NAT Gateway if privately access of EC2 to S3 bucket is needed. 

AWS Network firewall protects your entire Amazon VPC from Layer 3 to 7. Inspect traffic in any direction like VPC to VPC, Egress to Internet, Ingress from Internet, In/Out DX and S2S connections. Internally the AWS Network Firewall uses the AWS Gateway Load Balance and rules can be centrally managed from AWS Firewall manager to apply to many VPCs. Has active flow detection to protect against threats with intrusion-prevention similar to GLB. Has Traffic filtering and can perform Allow, drop or alert for traffic that matches the rules.

ENA Or ELastic Network Adapter is used for Enhanced Networking (SR-IOV) which provides higher bandwidth, higher PPS, lower latency. Gives upto 100Gbps speed. Use Intel 82599 for upto 10 Gbps and its LEGACY.

EFA or Elastic Fabric Adapter (EFA, its like ENA on steroids, speed is like you're connected to a supercomputer.) is an improved ENA for HPC, only works for Linux. Great for inter-node communications, tightly coupled workloads, leverages Message Passing Interface (MPI) standard. Bypasses the underlying Linux OS to provide low-latency, reliable transport.

Automation and Orchestration will use AWS Batch that supports multi-node parallel jobs enabling us to run single jobs spanning multiple EC2 instances. AWS Parallel Cluster is an open-source management tool to deploy HPC on AWS, configured with textfiles, maybe with parameters that run with EFA. Automates creation of VPC, subnet, cluster type and instance types.