Deploying multiple applications/Services will require to communicate with each other. Services communicating directly with each other is all synchronous communication with no middleware. Services communicatitng to a queue is called asynchronous communication with a queue as middleware. Issue with sync comms is that if one services has a traffic spike, the whole app would crash. Async comms will decouple services so that if one fails, the rest work as is since they are talking to a queue rather than the service. For decoupling you can use SQS: Queue model; SNS: pub/sub model; Kinesis: real-time streaming model.

A queue basically takes messages from services called producers (like process a video or order), then services called consumers will pull the messages from the queue from the other end, once the message is processed, it gets deleted from the queue.

Amazon SQS - standard queue is the oldest offering (10 years old) and is a fully managed service used to decouple applications. SQS provides unlimited throughput, unlimited number of messages in queue, default retention of messages is 4 days and max 14 days, low latency of about <10ms on pub and receive. Limitation of 256KB per message sent. Can have duplicate messages (at least once delivery, occasionally), can have out of order messages (best effort ordering). Also provides FIFO type of queue as well.

Message is produced to SQS using the SDK (SendMessage API). The message is persisted in SQS until a consumer deletes it. Example send an order to be processed with custom attributes like order id, customer id etc. Message is consumed by consumers (e.g EC2, servers, Lambda), Consumer polls SQS for messages (receive upto 10 messages at a time) then process the messages (e.g insert an order into RDS) and finally delete the messages using the DeleteMessage API after processing. Multiple consumers receive and process messages in parallel. At least once delivery and best-effort message ordering is applied here. We can scale consumers horizontally to improve throughput of processing.

Using SQS with ASG is a great strategy with CloudWatch Metric attached to the SQS queue. CloudWatch will check for queue length (using ApproximateNumberOfMessages metric query) and will sound a CloudWatch alarm if the queue length breaches the limit, causing the ASG to increase the number of instances to manage the traffic spike or workload.

Another great usecase is to decouple a Front web-app for video processing into two ASG groups rather than using a single instance with ASG. The front-end layer of the app would take the requests (present in its own ASG and sized accordingly) and forward them to SQS which would then be received by the backend layer of the app for video processing in its own ASG (this one can have a GPU optionally) and then the processed video is stored in S3.

For security, SQS has encryption, using HTTPS API for in-flight and KMS keys at-rest (SSE-SQS or SSE-KMS). Client-side encryption if the client wants to perform encryption/decryption themselves (not supported out of the box). Access control's managed using IAM policies to regulate access to the SQS API. SQS Access Policies (similar to S3 bucket policies) are useful for cross-account access to SQS queues and are useful for alowing other services (SNS, S3..) to write to an SQS queue.

When a message is polled (pulled) by a consumer, it becomes invisible to other consumers. By default, the visibility timeout is 30 seconds meaning the message has 30 seconds to be processed. After the timeout is over and the message has not been deleted then it is put back into the queue (becomes visible again) to be polled by the same or different consumer. If the message gets polled again then it will be processed twice (causing duplicates). A consumer could prevent this by calling the ChangeMessageVisibility API to get more time. If visibility timeout is high (hours), and consumer crashes, re-processing will take time. If visibility is too low (seconds), we get duplicates. Best practice is to set the timeout to moderate time enough for the consumer to process and allow the consumer to call the ChangeMessageVisibility API if it needs more time.

When a consumer requests messages from the queue but there are none so it optionally waits for messages to arrive - this is called Long Polling. Long polling decreases the number of API calls made to SQS while increasing the efficiency and reducing latency of your application. Preferred wait time is between 1 to 20 seconds with 20 being preferred most. Long polling is preferable to Short Polling and Long polling can be enabled at the API level or queue level using WaitTimeSeconds.

SQS - FIFO Queue means ordering of messages in the queue. Like if producer sends messages in order [4,3,2,1] then the consumer polled messages would be in order [4,3,2,1]. Limited throughput with 300 msg/s without batching and 3000 msg/s with. Exactly-once send capability and removes all duplicates sent within the 5 minute range using deduplication ID. Messages are processed in order and ordering is done via Message Group ID (all messages in the same group are ordered) - mandatory parameter. The name of the queue must end with .fifo to use the FIFO ability.

Another use case of SQS is to use it as a buffer, suppose you have a e-commerce website, and on black Friday there are many transactions being done and all of them have to be written to a database (e.g RDS, Aurora, DynamoDB). Too many transactions needed to be inserted too fast may result in some of them being lost due to overload on DBs. Solution is to use SQS queue as middleware, the ASG would enqueue the transaction it receives as client request, sends it to SQS, give it to another ASG for dequeue and then the ASG writes inserts it to the DB. With SQS infinite throughput, this means all transaction will be written NMW.

Also remember whenever exam says something about decoupling or sudden spike load or timeout and need to scale really fast then do look at SQS queue.

Amazon SNS is a service which is used to decouple application using pub/sub models, the producers will publish messages to a topic in the SNS rather than multiple services and the topic can have many subscribers (services to receive those messages in topic), these could be email, SMS/ mobile notifications, or custom HTTP(S) endpoints, can also be AWS services like Kinesis Data Firehose, Lambda and SQS. Publishers can be S3 events, ASG notifications, CloudWatch Alarms, CloudFormation state changes, AWS DB (RDS, Dynamo) Events.

There is a direct publish for mobile SDK as well which creates a platform application, platform endpoint, publish to the platform endpoint, works with Google GCM, Apple APNS, Amazon ADM. A subscriber can receive all the messages or a message filtering policy can be used to for a subscriber to only receive certain messages.

SNS + SQS Fanout pattern is where a service pushes once to SNS topic, and multiple SQS queues can subscribe to that topic, this approach is fully decoupled and provides no data loss, SQS allows for data persistence, delayed processing and retries of work, can add more SQS queues as subscribers overtime. SQS queues in region a can be written into by SNS in region b if the SQS access policies allow SNS to write, cross-regionally as well. e.g is when an S3 event is published to a topic, and is received by multiple subscribers like SQS queues, Lambda Function to execute some code in response to the event or a service sends it topic and topic to kinesis and it process and pushed to S3/redshift.

SNS and SQS FIFO always end with .fifo and has same features, like ordering of messages and deduplication using ID or content. Subscribers of SNS fifo can only be SQS (Standard and FIFO). Its used when you require fan+out plus ordering plus deduplication, where a service publishes to SNS FIFO topic, and subscribers are SQS FIFO queues, forwarding to their respective services.

Message filtering policy is attached to SNS at subscription level so only messages with certain properties are sent (pushed) to subscribers. Suppose there is a ordering service for a e-commerce site, a order is placed with order id, price, quantity and state etc. This order is published by the buying service to SNS topic, now there can be multiple subscribers with message filter policy attached for each subscription in SNS, like a SQS queue which only receives orders with state placed, another SQS queue which receives the orders from topic with state cancelled, an email subscription receives messages for cancelled orders as well, and SQS queue receives messages for declined orders.


Amazon Kinesis Data Streams is used collect and store real-time data received from producers and sent to consumers for further processing. Real-time data can be click streams, IoT devices or prod-server metrics/logs, producers can be applications (coded by you) or you can install a kinesis agent in your server and it will act as a producer, consumers can be applications, Lambda Functions, Amazon Data Firehose or managed service Apache Flink to perform analytics on that data.

Kinesis Data Streams has data retention of upto 365 days, you have the ability to have your data re-processed by consumers due to the retention, data can't be deleted from kinesis until it expires and max data size is 1 MB (tho typical use case is lots of small real-time data). Data ordering guarantee for data with the same partition ID, at rest KMS encryption, in-flight HTTPS encryption. Use Kinesis Producer Library (KPL) to write an optimized producer application and Kinesis Client Library (KCL) to write an optimized consumer application.

Two types of capacity modes for kinesis data streams, Provisioned mode is where you choose number of shards, each shard provides 1MB/s of input (write) or 1000 records per second and each shard get 2 MB/s output (read) or 2000 records p/s (20 shards mean 20MB/s write and 40MB/s read). Scale manually to increase or decrease the number of shards and you pay per shard provisioned per hour. Provisioned mode is where you don't need to provision or manage the capacity, default capacity provisioned (4MB/s in or 4000 records per second) scales automatically based on observed throughput peak during the last 30 days. Pay per stream per hour & data in/out per GB.

Important note: you can only double shards in one operation, so if you have 2, you can go scale to 4 max, then do it again to go further.


Amazon Data Firehose is used to send data received from (sources) producers and then accumulated to a buffer and the buffer will be flushed once in a while to batch write into many target destinations. Producers can be applications (coded by you), clients (pc, mobile), SDK, Kinesis Agent, Kinesis Data Streams, Amazon CloudWatch (Logs & Events), and AWS IoT. Data sent in records of 1MB max to Firehose, you can optionally use Lambda function to transform data or record (e.g filter, uncompress, convert (e.g Convert JSON to ORC/Parquet for efficient quering.)) and then batch write the data to AWS destinations such S3, RedShift, AWS OpenSearch. You can write is to 3rd party destination like DataDog, Splunk, MongoDB or custom destinations HTTP Endpoint to send it anywhere you want. You also have the option to write all or just the failed data to an S3 backup bucket. Firehose has auto scaling, its serverless and you pay for what you use.

Firehose is NEAR real-time with buffering capability based on size/time. Supports CSV, JSON, Parquet, Avro, Raw Text, Binary Data. Conversions to Parquet/ORC, compressions with gzip/snappy. Custom data transformation using AWS Lambda (e.g CSV to JSON).

Difference is that Kinesis Data Streams is used for streaming data collection, have to write own producer & consumer code, Its real-time, provisioned/on-demand mode, Data retention upto 365bdays and replay (re-process) capability.
WHILE Kinesis Data Firehose is used to load streaming data into S3/RedShift/OpenSearch/3rd Party/Custom HTTP, Fully managed, Near real-time, auto scaling, no data storage, doesn't support replay.

